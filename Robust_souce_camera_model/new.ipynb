{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Number of batches: 1440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'create_patch_dataloader.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 249\u001b[0m\n\u001b[0;32m    246\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Example usage of the print_images_in_grid function\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m \u001b[43mprint_images_in_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Train the models\u001b[39;00m\n\u001b[0;32m    252\u001b[0m train_feature_extractor_and_pdn(feature_extractor, PDN_model, dataloader, device)\n",
      "Cell \u001b[1;32mIn[1], line 237\u001b[0m, in \u001b[0;36mprint_images_in_grid\u001b[1;34m(dataloader, num_images)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_images_in_grid\u001b[39m(dataloader, num_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m49\u001b[39m):\n\u001b[1;32m--> 237\u001b[0m     images, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    238\u001b[0m     images \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m0\u001b[39m:num_images]\n\u001b[0;32m    239\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'create_patch_dataloader.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import random\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to extract patches from images\n",
    "def extract_patches(image, patch_size=64, stride=32):\n",
    "    patches = []\n",
    "    _, h, w = image.shape\n",
    "    for i in range(0, h - patch_size + 1, stride):\n",
    "        for j in range(0, w - patch_size + 1, stride):\n",
    "            patch = image[:, i:i+patch_size, j:j+patch_size]\n",
    "            patches.append(patch)\n",
    "    return torch.stack(patches)\n",
    "\n",
    "# Custom Dataset to load patches\n",
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, patch_size=64, stride=32):\n",
    "        self.dataset = datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        patches = extract_patches(image, patch_size=self.patch_size, stride=self.stride)\n",
    "        return patches, label\n",
    "\n",
    "# Function to create DataLoader\n",
    "def create_patch_dataloader(data_dir, batch_size=32, num_workers=4, patch_size=64, stride=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset = PatchDataset(root_dir=data_dir, transform=transform, patch_size=patch_size, stride=stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=lambda x: (torch.cat([item[0] for item in x], dim=0), [item[1] for item in x]))\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# Example usage\n",
    "data_dir = 'D:/Forchheim - Copy'  # Ensure this path is correct\n",
    "dataloader = create_patch_dataloader(data_dir, batch_size=1)\n",
    "\n",
    "# Print number of images loaded\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Function for PGD attack\n",
    "def pgd_attack(model, images, labels, eps=0.3, alpha=2/255, iters=40):\n",
    "    adv_images = images.clone().detach().requires_grad_(True)\n",
    "    for _ in range(iters):\n",
    "        outputs = model(adv_images)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = adv_images.grad.data\n",
    "        adv_images = adv_images + alpha * grad.sign()\n",
    "        adv_images = torch.clamp(adv_images, images - eps, images + eps)\n",
    "        adv_images = torch.clamp(adv_images, 0, 1)\n",
    "        adv_images = adv_images.detach().requires_grad_(True)\n",
    "    return adv_images\n",
    "\n",
    "# Define the PDN (Patch Discriminator Network)\n",
    "class PDN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PDN, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Define the Feature Extractor using ResNet-18\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.features(x).squeeze()\n",
    "\n",
    "# Initialize models\n",
    "feature_extractor = FeatureExtractor()\n",
    "PDN_model = PDN()\n",
    "\n",
    "# Train the Feature Extractor and PDN with patches\n",
    "def train_feature_extractor_and_pdn(feature_extractor, pdn_model, patch_dataloader, device):\n",
    "    feature_extractor.to(device)\n",
    "    pdn_model.to(device)\n",
    "    \n",
    "    # Optimizers and loss functions\n",
    "    feature_optimizer = optim.Adam(feature_extractor.parameters(), lr=0.001)\n",
    "    pdn_optimizer = optim.Adam(pdn_model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(feature_optimizer, gamma=0.97)\n",
    "    loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    # Training feature extractor\n",
    "    for epoch in range(20):\n",
    "        feature_extractor.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for patches, _ in patch_dataloader:\n",
    "            patches = patches.to(device)\n",
    "            features = feature_extractor(patches)\n",
    "            \n",
    "            # Forward pass through PDN model\n",
    "            reconstructed_patches = pdn_model(patches)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(reconstructed_patches, patches)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backpropagation\n",
    "            feature_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            feature_optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Feature Extractor Epoch [{epoch + 1}/20], Loss: {total_loss / len(patch_dataloader):.4f}\")\n",
    "\n",
    "    # Training PDN model\n",
    "    for epoch in range(10):\n",
    "        pdn_model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for patches, _ in patch_dataloader:\n",
    "            patches = patches.to(device)\n",
    "            \n",
    "            # Generate adversarial examples\n",
    "            adv_patches = pgd_attack(pdn_model, patches, labels=torch.zeros(patches.size(0), dtype=torch.long).to(device), eps=0.3, alpha=2/255, iters=40)\n",
    "            \n",
    "            # Forward pass through PDN model\n",
    "            reconstructed_patches = pdn_model(patches)\n",
    "            adv_reconstructed_patches = pdn_model(adv_patches)\n",
    "            \n",
    "            # Compute loss on clean and adversarial examples\n",
    "            loss = (loss_fn(reconstructed_patches, patches) + loss_fn(adv_reconstructed_patches, adv_patches)) / 2\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backpropagation\n",
    "            pdn_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            pdn_optimizer.step()\n",
    "        \n",
    "        print(f\"PDN Model Epoch [{epoch + 1}/10], Loss: {total_loss / len(patch_dataloader):.4f}\")\n",
    "\n",
    "# Function for major voting\n",
    "def major_voting(patches, model, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        patch_labels = []\n",
    "        for patch in patches:\n",
    "            patch = patch.unsqueeze(0).to(device)\n",
    "            features = model(patch)\n",
    "            # Assuming a classifier is used to predict the class from features\n",
    "            label = torch.argmax(features, dim=1).item()\n",
    "            patch_labels.append(label)\n",
    "        \n",
    "        # Aggregate results using majority voting\n",
    "        most_common_label, _ = Counter(patch_labels).most_common(1)[0]\n",
    "        return most_common_label\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(patches_per_image, model, device, true_labels):\n",
    "    image_level_labels = []\n",
    "    for patches in patches_per_image:\n",
    "        predicted_label = major_voting(patches, model, device)\n",
    "        image_level_labels.append(predicted_label)\n",
    "    \n",
    "    # Evaluate accuracy\n",
    "    accuracy = np.mean([pred == true_label for pred, true_label in zip(image_level_labels, true_labels)])\n",
    "    print(f\"Image Level Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Function to print images in a 7x7 grid\n",
    "def print_images_in_grid(dataloader, num_images=49):\n",
    "    images, _ = next(iter(dataloader))\n",
    "    images = images[0:num_images]\n",
    "    images = images.permute(0, 2, 3, 1)\n",
    "    images = (images * 255).byte().cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(7, 7, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i])\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage of the print_images_in_grid function\n",
    "print_images_in_grid(dataloader)\n",
    "\n",
    "# Train the models\n",
    "train_feature_extractor_and_pdn(feature_extractor, PDN_model, dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
