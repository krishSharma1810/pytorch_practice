{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import random\n",
    "from collections import Counter\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Custom Dataset class\n",
    "class ForchheimDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Load all image paths and corresponding labels\n",
    "        for outer_label in os.listdir(root_dir):\n",
    "            outer_dir = os.path.join(root_dir, outer_label)\n",
    "            if os.path.isdir(outer_dir):\n",
    "                for inner_label in os.listdir(outer_dir):\n",
    "                    inner_dir = os.path.join(outer_dir, inner_label)\n",
    "                    if os.path.isdir(inner_dir):\n",
    "                        for image_name in os.listdir(inner_dir):\n",
    "                            if image_name.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                                self.image_paths.append(os.path.join(inner_dir, image_name))\n",
    "                                # Use a combined label from outer and inner folder names\n",
    "                                combined_label = f\"{outer_label}_{inner_label}\"\n",
    "                                self.labels.append(combined_label)\n",
    "\n",
    "        if len(self.image_paths) == 0:\n",
    "            raise ValueError(f\"No images found in {root_dir}. Please check the directory path and structure.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Function to load dataset\n",
    "def load_forchheim_dataset(data_dir, batch_size=32, num_workers=4):\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Resize images to a consistent size\n",
    "        transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize based on ImageNet stats\n",
    "    ])\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = ForchheimDataset(root_dir=data_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "# Example usage\n",
    "data_dir = 'D:\\\\Forchheim'  # Replace with the path to your dataset\n",
    "batch_size = 32\n",
    "dataloader = load_forchheim_dataset(data_dir, batch_size)\n",
    "\n",
    "# Iterate through the dataloader\n",
    "for images, labels in dataloader:\n",
    "    print(images.size(), labels)  # Example: torch.Size([32, 3, 256, 256]) ['0_0', '0_1', ...]\n",
    "    break  # Just to demonstrate, remove this in your actual training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, images, labels, eps=0.3, alpha=2/255, iters=40):\n",
    "    adv_images = images.clone().detach().requires_grad_(True)\n",
    "    for _ in range(iters):\n",
    "        outputs = model(adv_images)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = adv_images.grad.data\n",
    "        adv_images = adv_images + alpha * grad.sign()\n",
    "        adv_images = torch.clamp(adv_images, images - eps, images + eps)\n",
    "        adv_images = torch.clamp(adv_images, 0, 1)\n",
    "        adv_images = adv_images.detach().requires_grad_(True)\n",
    "    return adv_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the PDN (Patch Discriminator Network)\n",
    "class PDN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PDN, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Feature Extractor using ResNet-18\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.features(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize models, optimizer, and loss function\n",
    "learning_rate = 0.001\n",
    "feature_extractor = FeatureExtractor().to(device)\n",
    "PDN_model = PDN().to(device)\n",
    "optimizer = optim.Adam(list(feature_extractor.parameters()) + list(PDN_model.parameters()), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "feature_extraction_epochs = 20\n",
    "pdn_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_feature_extractor_and_pdn(feature_extractor, pdn_model, train_loader, device):\n",
    "    feature_extractor.to(device)\n",
    "    pdn_model.to(device)\n",
    "    \n",
    "    # Optimizers and loss functions\n",
    "    feature_optimizer = optim.Adam(feature_extractor.parameters(), lr=0.001)\n",
    "    pdn_optimizer = optim.Adam(pdn_model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(feature_optimizer, gamma=0.97)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Training feature extractor\n",
    "    for epoch in range(20):\n",
    "        feature_extractor.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for images in train_loader:\n",
    "            images = images[0].to(device)\n",
    "            features = feature_extractor(images)\n",
    "            \n",
    "            # Forward pass through PDN model\n",
    "            reconstructed_images = pdn_model(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(reconstructed_images, images)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backpropagation\n",
    "            feature_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            feature_optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        print(f\"Feature Extractor Epoch [{epoch + 1}/20], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Training PDN model\n",
    "    for epoch in range(10):\n",
    "        pdn_model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for images in train_loader:\n",
    "            images = images[0].to(device)\n",
    "            \n",
    "            # Generate adversarial examples\n",
    "            adv_images = pgd_attack(pdn_model, images, labels=torch.zeros(images.size(0), dtype=torch.long).to(device), eps=0.3, alpha=2/255, iters=40)\n",
    "            \n",
    "            # Forward pass through PDN model\n",
    "            reconstructed_images = pdn_model(images)\n",
    "            adv_reconstructed_images = pdn_model(adv_images)\n",
    "            \n",
    "            # Compute loss on clean and adversarial examples\n",
    "            loss = (loss_fn(reconstructed_images, images) + loss_fn(adv_reconstructed_images, adv_images)) / 2\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backpropagation\n",
    "            pdn_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            pdn_optimizer.step()\n",
    "        \n",
    "        print(f\"PDN Model Epoch [{epoch + 1}/10], Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_model_with_adversarial(feature_extractor, \u001b[43mpdn_model\u001b[49m, patch_loader, optimizer, loss_fn, device, feature_extraction_epochs, pdn_epochs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pdn_model' is not defined"
     ]
    }
   ],
   "source": [
    "train_feature_extractor_and_pdn(feature_extractor, PDN_model, patch_loader, optimizer, loss_fn, device, feature_extraction_epochs, pdn_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def major_voting(patches, model, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        patch_labels = []\n",
    "        for patch in patches:\n",
    "            patch = patch.unsqueeze(0).to(device)\n",
    "            features = model(patch)\n",
    "            # Assuming a classifier is used to predict the class from features\n",
    "            label = torch.argmax(features, dim=1).item()\n",
    "            patch_labels.append(label)\n",
    "        \n",
    "        # Aggregate results using majority voting\n",
    "        most_common_label, _ = Counter(patch_labels).most_common(1)[0]\n",
    "        return most_common_label\n",
    "\n",
    "def evaluate_model(patches_per_image, model, device):\n",
    "    image_level_labels = []\n",
    "    for patches in patches_per_image:\n",
    "        predicted_label = major_voting(patches, model, device)\n",
    "        image_level_labels.append(predicted_label)\n",
    "    \n",
    "    # Evaluate accuracy\n",
    "    # Assuming `true_labels` is a list of true labels for each image\n",
    "    accuracy = np.mean([pred == true_label for pred, true_label in zip(image_level_labels, true_labels)])\n",
    "    print(f\"Image Level Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
