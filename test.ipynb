{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_blur_simple(file, r=100):\n",
    "  img = Image.open(file).convert('RGB')\n",
    "  new_img = img.copy()\n",
    "  width, height = img.size\n",
    "  \n",
    "  for x in range(r, width - r):\n",
    "    for y in range(r, height - r):\n",
    "      # Initialize sum variables for each color channel (Red, Green, Blue)\n",
    "      sum_red = 0\n",
    "      sum_green = 0\n",
    "      sum_blue = 0\n",
    "      # Iterate through surrounding pixels within blur radius\n",
    "      for dx in range(-r, r + 1):\n",
    "        for dy in range(-r, r + 1):\n",
    "          # Consider pixels within image boundaries\n",
    "          if 0 <= x + dx < width and 0 <= y + dy < height:\n",
    "            pixel = img.getpixel((x + dx, y + dy))\n",
    "            sum_red += pixel[0]\n",
    "            sum_green += pixel[1]\n",
    "            sum_blue += pixel[2]\n",
    "      # Calculate average for each color channel\n",
    "      avg_red = sum_red // ((2 * r + 1) ** 2)\n",
    "      avg_green = sum_green // ((2 * r + 1) ** 2)\n",
    "      avg_blue = sum_blue // ((2 * r + 1) ** 2)\n",
    "      # Set the averaged pixel value in the new image\n",
    "      new_img.putpixel((x, y), (avg_red, avg_green, avg_blue))\n",
    "  \n",
    "  return new_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m img1\u001b[38;5;241m=\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(file)\n\u001b[0;32m      4\u001b[0m img1\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m----> 5\u001b[0m img\u001b[38;5;241m=\u001b[39m\u001b[43mbox_blur_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m img\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mbox_blur_simple\u001b[1;34m(file, r)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dy \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m-\u001b[39mr, r \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     15\u001b[0m   \u001b[38;5;66;03m# Consider pixels within image boundaries\u001b[39;00m\n\u001b[0;32m     16\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m dx \u001b[38;5;241m<\u001b[39m width \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m+\u001b[39m dy \u001b[38;5;241m<\u001b[39m height:\n\u001b[1;32m---> 17\u001b[0m     pixel \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetpixel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     sum_red \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pixel[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m     sum_green \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pixel[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\PIL\\Image.py:1612\u001b[0m, in \u001b[0;36mImage.getpixel\u001b[1;34m(self, xy)\u001b[0m\n\u001b[0;32m   1610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyaccess:\n\u001b[0;32m   1611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyaccess\u001b[38;5;241m.\u001b[39mgetpixel(xy)\n\u001b[1;32m-> 1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim\u001b[38;5;241m.\u001b[39mgetpixel(\u001b[38;5;28mtuple\u001b[39m(xy))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file=\"D05_img_telegram_0139.jpg\"\n",
    "    img1=Image.open(file)\n",
    "    img1.show()\n",
    "    img=box_blur_simple(file)\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def box_blur_gpu(file, r=1):\n",
    "  # Open image and convert to RGB\n",
    "  img = Image.open(file).convert('RGB')\n",
    "  width, height = img.size\n",
    "\n",
    "  # Convert image to a NumPy array (CHW format)\n",
    "  img_arr = np.array(img).astype(np.float32)\n",
    "  img_arr = img_arr.transpose((2, 0, 1))  # Channels first (CHW)\n",
    "\n",
    "  # Convert NumPy array to PyTorch tensor and move to GPU if available\n",
    "  img_tensor = torch.from_numpy(img_arr).unsqueeze(0)  # Add batch dimension\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  img_tensor = img_tensor.to(device)\n",
    "\n",
    "  # Apply box blur using a kernel on the GPU (implementation details omitted)\n",
    "  blurred_tensor = box_blur_kernel(img_tensor, r, device)  # Replace with your GPU kernel implementation\n",
    "\n",
    "  # Convert blurred tensor back to NumPy array and reshape to image format (HWC)\n",
    "  blurred_arr = blurred_tensor.squeeze(0).cpu().numpy().transpose((1, 2, 0))\n",
    "\n",
    "  # Convert blurred NumPy array back to PIL Image\n",
    "  blurred_img = Image.fromarray(blurred_arr.astype(np.uint8))\n",
    "\n",
    "  return blurred_img\n",
    "\n",
    "# This part requires implementation (specific to your chosen deep learning framework)\n",
    "def box_blur_kernel(img_tensor, r, device):\n",
    "  # Implement the box blur logic using PyTorch operations on the GPU (e.g., convolutions)\n",
    "  # This function should take the image tensor, blur radius, and device as input\n",
    "  # and return the blurred tensor on the specified device.\n",
    "  # ... (implementation details for box blur kernel using PyTorch)\n",
    "  raise NotImplementedError(\"Please implement box_blur_kernel using PyTorch operations on GPU\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
